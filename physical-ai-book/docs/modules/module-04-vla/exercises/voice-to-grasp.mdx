---
sidebar_position: 5
title: 'Voice-to-Grasp Exercise'
description: 'Implement a voice-commanded grasping task'
---

# Voice-to-Grasp Exercise

## Objective

In this exercise, you will implement a complete voice-commanded grasping system that combines speech recognition, object detection, and robotic manipulation. This exercise integrates concepts from all three chapters of Module 4.

## Prerequisites

Before starting this exercise, ensure you have:

- Completed Chapters 1-3 of Module 4
- Access to a robotic platform (physical or simulated) with grasping capabilities
- Access to a camera for visual input
- Microphone for voice input
- OpenAI API key configured

## Exercise Overview

You will build a system that:
1. Listens for voice commands like "Grasp the red cup on the table"
2. Processes the visual scene to identify objects
3. Maps the voice command to a specific object in the scene
4. Executes a grasping action on the identified object

## Step 1: Set Up the Voice Command Processor

First, implement the voice command processor from Chapter 1:

```python
import speech_recognition as sr
import rospy
from std_msgs.msg import String
import json

class VoiceCommandProcessor:
    def __init__(self):
        self.recognizer = sr.Recognizer()
        self.microphone = sr.Microphone()

        # Calibrate for ambient noise
        with self.microphone as source:
            self.recognizer.adjust_for_ambient_noise(source)

        # Initialize ROS publisher
        self.command_pub = rospy.Publisher('/voice_command', String, queue_size=10)

    def listen_for_command(self):
        """Listen for a voice command and return the text"""
        with self.microphone as source:
            print("Listening for command...")
            audio = self.recognizer.listen(source)

        try:
            # Use Whisper API for speech recognition
            text = self.recognizer.recognize_whisper_api(
                audio,
                api_key=os.getenv("OPENAI_API_KEY"),
                model="whisper-1"
            )
            return text
        except sr.RequestError as e:
            print(f"API error: {e}")
            return None
        except sr.UnknownValueError:
            print("Could not understand audio")
            return None

    def process_command(self, command_text):
        """Process the command text and extract grasp parameters"""
        # Extract object description from command
        # This is a simplified version - in practice, you'd use more sophisticated NLP
        command_lower = command_text.lower()

        # Extract color and object type
        colors = ["red", "blue", "green", "yellow", "white", "black"]
        objects = ["cup", "bottle", "box", "book", "mug", "can"]

        target_color = None
        target_object = None

        for color in colors:
            if color in command_lower:
                target_color = color
                break

        for obj in objects:
            if obj in command_lower:
                target_object = obj
                break

        return {
            "color": target_color,
            "object_type": target_object,
            "full_command": command_text
        }
```

## Step 2: Implement Vision-Language Processor

Next, implement the vision-language processor from Chapter 3:

```python
import openai
import cv2
import numpy as np
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import base64

class VisionLanguageProcessor:
    def __init__(self):
        openai.api_key = os.getenv("OPENAI_API_KEY")
        self.client = openai.OpenAI()
        self.bridge = CvBridge()

    def identify_target_object(self, image, voice_command):
        """Identify the target object based on voice command and visual input"""
        cv_image = self.bridge.imgmsg_to_cv2(image, "bgr8")
        _, buffer = cv2.imencode('.jpg', cv_image)
        image_base64 = base64.b64encode(buffer).decode('utf-8')

        prompt = f"""
        You are a vision-language assistant for a robot tasked with grasping objects.
        The user said: "{voice_command}"

        Analyze the image to find the object that matches the description.
        Return a JSON object with:
        - target_object: Description of the object to grasp
        - color: Color of the target object
        - object_type: Type of the target object
        - coordinates: Center coordinates (x, y) of the target object in the image
        - confidence: Confidence level (0-1) that this is the correct object
        """

        response = self.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=500
        )

        return json.loads(response.choices[0].message.content)
```

## Step 3: Create the Grasping Controller

Now implement the grasping controller:

```python
import rospy
from geometry_msgs.msg import Point
from std_msgs.msg import String
import json

class GraspingController:
    def __init__(self):
        rospy.init_node('voice_grasping_controller')

        # Publisher for grasping commands
        self.grasp_pub = rospy.Publisher('/grasp_command', String, queue_size=10)

        # Subscriber for vision-language results
        self.vla_sub = rospy.Subscriber('/vla_result', String, self.vla_callback)

        # Store latest grasp parameters
        self.latest_grasp_params = None

    def vla_callback(self, msg):
        """Process vision-language analysis results"""
        try:
            data = json.loads(msg.data)
            if data.get('confidence', 0) > 0.7:  # Only grasp if confidence is high enough
                self.execute_grasp(data)
            else:
                rospy.logwarn(f"Low confidence ({data.get('confidence', 0)}) for grasp target")
        except json.JSONDecodeError:
            rospy.logerr("Could not parse VLA result")

    def execute_grasp(self, grasp_data):
        """Execute grasping action based on vision-language analysis"""
        grasp_command = {
            "action": "grasp",
            "target_object": grasp_data['target_object'],
            "coordinates": grasp_data['coordinates'],
            "object_type": grasp_data['object_type'],
            "color": grasp_data['color']
        }

        command_msg = String()
        command_msg.data = json.dumps(grasp_command)
        self.grasp_pub.publish(command_msg)

        rospy.loginfo(f"Executing grasp for {grasp_data['color']} {grasp_data['object_type']}")
```

## Step 4: Integrate All Components

Create the main integration node:

```python
#!/usr/bin/env python3
import rospy
from sensor_msgs.msg import Image
from std_msgs.msg import String
import json
import threading

class VoiceGraspingSystem:
    def __init__(self):
        rospy.init_node('voice_grasping_system')

        # Initialize components
        self.voice_processor = VoiceCommandProcessor()
        self.vision_processor = VisionLanguageProcessor()
        self.grasp_controller = GraspingController()

        # Publishers and subscribers
        self.image_sub = rospy.Subscriber('/camera/rgb/image_raw', Image, self.image_callback)
        self.vla_result_pub = rospy.Publisher('/vla_result', String, queue_size=10)

        # Store latest image
        self.latest_image = None
        self.image_lock = threading.Lock()

        # Store voice command waiting for image
        self.pending_command = None

    def image_callback(self, msg):
        """Handle incoming image"""
        with self.image_lock:
            self.latest_image = msg

        # If there's a pending command, process it now
        if self.pending_command is not None:
            self.process_voice_command_with_image()

    def process_voice_command_with_image(self):
        """Process the pending voice command with the latest image"""
        if self.pending_command is None:
            return

        with self.image_lock:
            if self.latest_image is None:
                rospy.logwarn("No image available for voice command processing")
                return

            latest_image = self.latest_image
            command = self.pending_command

        # Process with vision-language system
        try:
            result = self.vision_processor.identify_target_object(latest_image, command)

            # Publish result for grasping controller
            result_msg = String()
            result_msg.data = json.dumps(result)
            self.vla_result_pub.publish(result_msg)

            rospy.loginfo(f"Processed voice command: {command}")
            self.pending_command = None  # Clear the pending command
        except Exception as e:
            rospy.logerr(f"Error processing voice command with image: {e}")
            self.pending_command = None

    def run(self):
        """Main run loop - continuously listen for voice commands"""
        rate = rospy.Rate(1)  # Check for new commands at 1 Hz

        while not rospy.is_shutdown():
            # Listen for a voice command
            command = self.voice_processor.listen_for_command()

            if command:
                rospy.loginfo(f"Heard command: {command}")

                # Check if it's a grasping command
                if any(keyword in command.lower() for keyword in ["grasp", "pick up", "grab", "take"]):
                    self.pending_command = command
                    rospy.loginfo("Waiting for image to process grasping command...")
                else:
                    rospy.loginfo("Command is not a grasping command, ignoring.")

            rate.sleep()

if __name__ == '__main__':
    system = VoiceGraspingSystem()
    try:
        system.run()
    except rospy.ROSInterruptException:
        pass
```

## Step 5: Testing the System

1. Launch your robot simulation (Isaac Sim or other platform)
2. Ensure camera and microphone are properly configured
3. Run the voice grasping system:
   ```bash
   rosrun your_package voice_grasping_system.py
   ```
4. Speak commands like:
   - "Grasp the red cup"
   - "Pick up the blue bottle"
   - "Grab the green box"

## Expected Outcomes

After completing this exercise, you should be able to:

1. Integrate voice recognition with visual object detection
2. Create a complete pipeline from speech to action
3. Handle spatial relationships in voice commands
4. Implement confidence-based decision making
5. Debug issues that arise when combining multiple AI systems

## Troubleshooting

- If voice recognition fails frequently, check microphone settings and ambient noise
- If object detection is inaccurate, ensure good lighting conditions
- If grasping fails, verify that the robot's kinematics and calibration are correct
- If the system is slow, consider optimizing the image encoding/decoding process

## Extension Challenges

1. Add support for more complex spatial relationships ("to the left of", "next to", etc.)
2. Implement a feedback system that confirms the target object before grasping
3. Add support for multiple objects in the same scene
4. Implement error recovery when the initial grasp attempt fails